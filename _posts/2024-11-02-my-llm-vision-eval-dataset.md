# Introducing My Personal Vision LLM Benchmark Dataset

In the rapidly evolving landscape of vision-capable Large Language Models (LLMs), having a consistent benchmark dataset is crucial for evaluation. This post introduces my small personal benchmark dataset that reflects real-world usage scenarios from my smartphone's camera roll.

The purpose of this is to have a post to refer back to when posting results of testing new models.

## Dataset Overview

Each image serves a specific purpose in testing different aspects of vision LLMs' capabilities. There's currently a bit much from Pokemon Go here, but on the other hand I still play and document achievements by taking screenshots so its easily half of my camera roll...

### 1. Game Interface Analysis
![Pokemon Go Collection Challenge](/assets/images/test1.png)
The first image showcases a Pokémon GO collection challenge screenshot, testing the model's ability to interpret gaming interfaces and recognize specific game elements.

### 2. Natural Scene Understanding
![Field with Power Lines](/assets/images/test2.jpg)
A landscape photograph featuring power lines against a natural backdrop, this image tests the model's capability to describe both natural and man-made elements in an outdoor setting.

### 3. Interactive Game Elements
![Pokemon Go Catch Scene](/assets/images/test3.png)
A Pokémon GO catch scene screenshot, challenging the model's ability to recognize dynamic gaming elements and specific game mechanics.

### 4. Text Recognition and Product Details
![Gainomax Product](/assets/images/test4.jpg)
A detailed shot of a Gainomax product, specifically chosen to test Optical Character Recognition (OCR) capabilities and product detail identification.

### 5. Pokedex Recognition
![Pokemon Go Pokedex](/assets/images/test5.png)
A Pokémon GO Pokédex screenshot, testing the model's ability to interpret structured game information.

### 6. Technical Documentation
![Electrical Saw](/assets/images/test6.jpg)
An electrical saw photograph with technical specifications, evaluating the model's capability to process and understand technical product information.

## Usage

This benchmark is my go-to set of images to throw at any vision-capable LLM to do a quick evaluation of:
- Accuracy of scene description
- Text recognition capabilities
- Understanding of technical specifications
- Game interface interpretation

By using a consistent set of images across different models, we can better understand their relative strengths and limitations in real-world applications.

